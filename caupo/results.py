"""
Auxiliary module for storage and easy calculation of statistics and metrics
that are provided by data generated by other modules, namely `caupo.cluster_tags`
"""

import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import ludovico

VALID_FREQUENCIES = [
    'daily',
    'weekly',
    'monthly',
]


def calculate_valid_entries(frequency: str, data: pd.DataFrame) -> pd.DataFrame:
    """Given raw result data, finds amount of valid entries"""

    assert frequency in VALID_FREQUENCIES, "Unknown frequency value"

    data = data.loc[data["frequency"] == frequency]
    data["valid_entries"] = data["sil_score"].apply(lambda x: "NaN" if str(x) == "None" else x).astype("float32")
    data.dropna()

    grouped_data = data[["algorithm", "embedder", "valid_entries"]].groupby(["algorithm", "embedder"])
    return grouped_data.count()


def calculate_average_silhouette(frequency: str, data: pd.DataFrame) -> pd.DataFrame:
    """Given raw result data, finds average silhouette data for a given frequency"""

    assert frequency in VALID_FREQUENCIES, "Unknown frequency value"

    data = data.loc[data["frequency"] == frequency]
    data["sil_score"] = data["sil_score"].apply(lambda x: "NaN" if str(x) == "None" else x).astype("float32")

    grouped_data = data[["algorithm", "embedder", "sil_score"]].groupby(["algorithm", "embedder"])
    return grouped_data.mean().sort_values(by=["sil_score"], ascending=False)


def calculate_average_davies_bouldin(frequency: str, data: pd.DataFrame) -> pd.DataFrame:
    """Given raw result data, finds average davies bouldin data for a given frequency"""

    assert frequency in VALID_FREQUENCIES, "Unknown frequency value"

    data = data.loc[data["frequency"] == frequency]
    data["db_score"] = data["db_score"].apply(lambda x: "NaN" if str(x) == "None" else x).astype("float32")

    grouped_data = data[["algorithm", "embedder", "db_score"]].groupby(["algorithm", "embedder"])
    return grouped_data.mean().sort_values(by=["db_score"], ascending=False)


def calculate_average_calinski_harabasz(frequency: str, data: pd.DataFrame) -> pd.DataFrame:
    """Given raw result data, finds average davies bouldin data for a given frequency"""

    assert frequency in VALID_FREQUENCIES, "Unknown frequency value"

    data = data.loc[data["frequency"] == frequency]
    data["ch_score"] = data["ch_score"].apply(lambda x: "NaN" if str(x) == "None" else x).astype("float32")

    grouped_data = data[["algorithm", "embedder", "ch_score"]].groupby(["algorithm", "embedder"])
    return grouped_data.mean().sort_values(by=["ch_score"], ascending=False)


def calculate_consolidated_data(frequency: str, data: pd.DataFrame) -> pd.DataFrame:
    """Given raw result data, calculates a consolidated dataframe"""

    assert frequency in VALID_FREQUENCIES, "Unknown frequency value"

    avg_silhouette_scores = calculate_average_silhouette(frequency, data.copy())
    valid_entries = calculate_valid_entries(frequency, data.copy())

    consolidated = pd.concat([avg_silhouette_scores, valid_entries], axis=1)

    max_entries_value = np.max(consolidated['valid_entries'].tolist())
    consolidated["weighted_score"] = (consolidated["sil_score"] * consolidated["valid_entries"]) / max_entries_value

    return consolidated.sort_values(by=["weighted_score", "sil_score"], ascending=False)


def consolidate_three_averages(frequency: str, data: pd.DataFrame) -> pd.DataFrame:
    """Given raw result data, consolidates the average of the three measurements"""

    assert frequency in VALID_FREQUENCIES, "Unknown frequency value"

    avg_silhouette_scores = calculate_average_silhouette(frequency, data.copy())
    avg_davies_bouldin = calculate_average_davies_bouldin(frequency, data.copy())
    avg_calinski_harabasz = calculate_average_calinski_harabasz(frequency, data.copy())

    consolidated = pd.concat(
        [avg_silhouette_scores, avg_davies_bouldin, avg_calinski_harabasz],
        axis=1
    ).round(3).reset_index().rename(
        columns={
            'embedder': 'Modelo',
            'algorithm': 'Algoritmo',
            'sil_score': 'Silueta',
            'db_score': 'Davies-Bouldin',
            'ch_score': 'Calinski-Harabasz',
        }
    )

    return consolidated.sort_values(by=["Silueta"], ascending=False)


def read_csv(file_path: Path) -> pd.DataFrame:
    """Given a path to a file, reads the file and returns a dataframe"""

    return pd.read_csv(file_path)


def main() -> None:
    """Read input arguments and calculates and returns results"""

    parser = argparse.ArgumentParser()
    parser.add_argument("--frequency", metavar="FREQUENCY", type=str, default="daily",
                        choices=VALID_FREQUENCIES)
    args = parser.parse_args()
    print(f"Received frequency `{args.frequency}`")

    file_path = Path(f"outputs/cluster_tags/{args.frequency}/results.csv")
    output_file_path = Path(f"outputs/cluster_tags/{args.frequency}/aggregated_results.csv")
    assert file_path.exists(), f"The file {file_path} does not exist"

    data = read_csv(file_path)

    # Get average of silhouette score
    consolidated_data = calculate_consolidated_data(args.frequency, data.copy())
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print("Avg. Silhouette Score & valid entries for each algorithm and embedding, over all entries")
        print(consolidated_data)
    consolidated_data.to_csv(output_file_path)

    # Get consolidated table with three measurements
    consolidated_three_averages_data = consolidate_three_averages(args.frequency, data.copy())
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print("Avg metrics for each algorithm and embedding, over all entries")
        print(consolidated_three_averages_data)
    print(f"Printing TeX table for Three averages with frequency={args.frequency}")
    table = ludovico.generate_comparison_for_two_columns(
        consolidated_three_averages_data,
        "Algoritmo",
        "Modelo",
        ["Silueta", "Davies-Bouldin", "Calinski-Harabasz"],
        add_hlines=True,
        data_highlight={
            'Silueta': 'max',
            'Davies-Bouldin': 'min',
            'Calinski-Harabasz': 'max',
        },
        table_width=0.5,
    )
    print(table)


if __name__ == "__main__":
    main()
